#!/usr/bin/env python3
"""
å°ç£é£Ÿå“è—¥ç‰©ç®¡ç†ç½² - é£Ÿå“ç‡Ÿé¤Šæˆåˆ†è³‡æ–™åº«æŠ“å–å™¨
æŠ“å– https://consumer.fda.gov.tw/Food/TFND.aspx?nodeID=178 çš„ç‡Ÿé¤Šè³‡æ–™
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import csv
import os
from datetime import datetime
import logging
from typing import List, Dict, Optional
import re

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FDANutritionScraper:
    """FDA ç‡Ÿé¤Šè³‡æ–™åº«æŠ“å–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æŠ“å–å™¨"""
        self.base_url = "https://consumer.fda.gov.tw/Food/TFND.aspx"
        self.session = requests.Session()
        
        # è¨­å®šè«‹æ±‚æ¨™é ­
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # é£Ÿå“åˆ†é¡å°æ‡‰
        self.food_categories = {
            'ç©€ç‰©é¡': 'A',
            'æ¾±ç²‰é¡': 'B', 
            'å …æœåŠç¨®å­é¡': 'C',
            'æ°´æœé¡': 'D',
            'è”¬èœé¡': 'E',
            'è—»é¡': 'F',
            'è‡é¡': 'G',
            'è±†é¡': 'H',
            'è‚‰é¡': 'I',
            'é­šè²é¡': 'J',
            'è›‹é¡': 'K',
            'ä¹³å“é¡': 'L',
            'æ²¹è„‚é¡': 'M',
            'ç³–é¡': 'N',
            'å—œå¥½æ€§é£²æ–™é¡': 'O',
            'èª¿å‘³æ–™åŠé¦™è¾›æ–™é¡': 'P',
            'ç³•é¤…é»å¿ƒé¡': 'Q',
            'åŠ å·¥èª¿ç†é£Ÿå“é¡': 'R'
        }
    
    def get_main_page(self) -> Optional[BeautifulSoup]:
        """ç²å–ä¸»é é¢"""
        try:
            params = {'nodeID': '178'}
            response = self.session.get(self.base_url, params=params)
            response.raise_for_status()
            
            # è¨­å®šç·¨ç¢¼
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            return soup
            
        except Exception as e:
            logger.error(f"ç²å–ä¸»é é¢å¤±æ•—: {e}")
            return None
    
    def extract_viewstate(self, soup: BeautifulSoup) -> Dict[str, str]:
        """æå– ASP.NET çš„ ViewState åƒæ•¸"""
        viewstate_data = {}
        
        # æå– __VIEWSTATE
        viewstate = soup.find('input', {'name': '__VIEWSTATE'})
        if viewstate:
            viewstate_data['__VIEWSTATE'] = viewstate.get('value', '')
        
        # æå– __VIEWSTATEGENERATOR
        viewstategenerator = soup.find('input', {'name': '__VIEWSTATEGENERATOR'})
        if viewstategenerator:
            viewstate_data['__VIEWSTATEGENERATOR'] = viewstategenerator.get('value', '')
        
        # æå– __EVENTVALIDATION
        eventvalidation = soup.find('input', {'name': '__EVENTVALIDATION'})
        if eventvalidation:
            viewstate_data['__EVENTVALIDATION'] = eventvalidation.get('value', '')
        
        return viewstate_data
    
    def search_foods(self, category: str = '', keyword: str = '', page: int = 1) -> List[Dict]:
        """æœå°‹é£Ÿå“è³‡æ–™"""
        try:
            # ç²å–ä¸»é é¢
            soup = self.get_main_page()
            if not soup:
                return []
            
            # æå– ViewState åƒæ•¸
            viewstate_data = self.extract_viewstate(soup)
            
            # æº–å‚™æœå°‹åƒæ•¸
            search_data = {
                '__VIEWSTATE': viewstate_data.get('__VIEWSTATE', ''),
                '__VIEWSTATEGENERATOR': viewstate_data.get('__VIEWSTATEGENERATOR', ''),
                '__EVENTVALIDATION': viewstate_data.get('__EVENTVALIDATION', ''),
                '__EVENTTARGET': '',
                '__EVENTARGUMENT': '',
                'ctl00$ContentPlaceHolder1$DropDownList1': category,
                'ctl00$ContentPlaceHolder1$TextBox1': keyword,
                'ctl00$ContentPlaceHolder1$Button1': 'æŸ¥è©¢'
            }
            
            # å¦‚æœæ˜¯åˆ†é ï¼Œè¨­å®šåˆ†é åƒæ•¸
            if page > 1:
                search_data['__EVENTTARGET'] = 'ctl00$ContentPlaceHolder1$GridView1'
                search_data['__EVENTARGUMENT'] = f'Page${page}'
            
            # ç™¼é€æœå°‹è«‹æ±‚
            response = self.session.post(self.base_url, data=search_data, params={'nodeID': '178'})
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            # è§£æçµæœ
            soup = BeautifulSoup(response.text, 'html.parser')
            return self.parse_search_results(soup)
            
        except Exception as e:
            logger.error(f"æœå°‹é£Ÿå“è³‡æ–™å¤±æ•—: {e}")
            return []
    
    def parse_search_results(self, soup: BeautifulSoup) -> List[Dict]:
        """è§£ææœå°‹çµæœ"""
        results = []
        
        try:
            # æ‰¾åˆ°çµæœè¡¨æ ¼
            table = soup.find('table', {'id': 'ctl00_ContentPlaceHolder1_GridView1'})
            if not table:
                return results
            
            # è§£æè¡¨æ ¼è¡Œ
            rows = table.find_all('tr')[1:]  # è·³éæ¨™é¡Œè¡Œ
            
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 5:
                    # æå–é€£çµ
                    link_cell = cells[1].find('a')
                    detail_url = None
                    if link_cell:
                        detail_url = 'https://consumer.fda.gov.tw/Food/' + link_cell.get('href', '')
                    
                    food_data = {
                        'æ•´åˆç·¨è™Ÿ': cells[0].get_text(strip=True),
                        'æ¨£å“åç¨±': cells[1].get_text(strip=True),
                        'ä¿—å': cells[2].get_text(strip=True),
                        'æ¨£å“è‹±æ–‡åç¨±': cells[3].get_text(strip=True),
                        'å…§å®¹ç‰©æè¿°': cells[4].get_text(strip=True),
                        'è©³ç´°é é¢URL': detail_url
                    }
                    
                    results.append(food_data)
            
        except Exception as e:
            logger.error(f"è§£ææœå°‹çµæœå¤±æ•—: {e}")
        
        return results
    
    def get_food_detail(self, detail_url: str) -> Optional[Dict]:
        """ç²å–é£Ÿå“è©³ç´°ç‡Ÿé¤Šè³‡è¨Š"""
        try:
            response = self.session.get(detail_url)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            return self.parse_food_detail(soup)
            
        except Exception as e:
            logger.error(f"ç²å–é£Ÿå“è©³ç´°è³‡è¨Šå¤±æ•—: {e}")
            return None
    
    def parse_food_detail(self, soup: BeautifulSoup) -> Optional[Dict]:
        """è§£æé£Ÿå“è©³ç´°è³‡è¨Š"""
        try:
            detail_data = {}
            
            # æ‰¾åˆ°è©³ç´°è³‡è¨Šè¡¨æ ¼
            tables = soup.find_all('table')
            
            for table in tables:
                rows = table.find_all('tr')
                
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        key = cells[0].get_text(strip=True)
                        value = cells[1].get_text(strip=True)
                        
                        if key and value:
                            detail_data[key] = value
            
            # æå–ç‡Ÿé¤Šæˆåˆ†
            nutrition_data = self.extract_nutrition_data(soup)
            detail_data['ç‡Ÿé¤Šæˆåˆ†'] = nutrition_data
            
            return detail_data
            
        except Exception as e:
            logger.error(f"è§£æé£Ÿå“è©³ç´°è³‡è¨Šå¤±æ•—: {e}")
            return None
    
    def extract_nutrition_data(self, soup: BeautifulSoup) -> Dict:
        """æå–ç‡Ÿé¤Šæˆåˆ†è³‡æ–™"""
        nutrition = {}
        
        try:
            # å°‹æ‰¾ç‡Ÿé¤Šæˆåˆ†è¡¨æ ¼
            tables = soup.find_all('table')
            
            for table in tables:
                rows = table.find_all('tr')
                
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        nutrient_name = cells[0].get_text(strip=True)
                        nutrient_value = cells[1].get_text(strip=True)
                        
                        # æ¸…ç†è³‡æ–™
                        if nutrient_name and nutrient_value:
                            # ç§»é™¤å–®ä½å’Œç‰¹æ®Šå­—ç¬¦
                            clean_value = re.sub(r'[^\d.]', '', nutrient_value)
                            if clean_value:
                                try:
                                    nutrition[nutrient_name] = float(clean_value)
                                except ValueError:
                                    nutrition[nutrient_name] = nutrient_value
            
        except Exception as e:
            logger.error(f"æå–ç‡Ÿé¤Šæˆåˆ†å¤±æ•—: {e}")
        
        return nutrition
    
    def scrape_all_foods(self, max_pages: int = 50) -> List[Dict]:
        """æŠ“å–æ‰€æœ‰é£Ÿå“è³‡æ–™"""
        all_foods = []
        
        try:
            logger.info("é–‹å§‹æŠ“å–æ‰€æœ‰é£Ÿå“è³‡æ–™...")
            
            # é€é æŠ“å–
            for page in range(1, max_pages + 1):
                logger.info(f"æ­£åœ¨æŠ“å–ç¬¬ {page} é ...")
                
                foods = self.search_foods(page=page)
                if not foods:
                    logger.info(f"ç¬¬ {page} é æ²’æœ‰è³‡æ–™ï¼Œåœæ­¢æŠ“å–")
                    break
                
                all_foods.extend(foods)
                
                # é¿å…è«‹æ±‚éæ–¼é »ç¹
                time.sleep(1)
            
            logger.info(f"ç¸½å…±æŠ“å–åˆ° {len(all_foods)} ç­†é£Ÿå“è³‡æ–™")
            return all_foods
            
        except Exception as e:
            logger.error(f"æŠ“å–æ‰€æœ‰é£Ÿå“è³‡æ–™å¤±æ•—: {e}")
            return all_foods
    
    def scrape_food_details(self, foods: List[Dict], max_details: int = 100) -> List[Dict]:
        """æŠ“å–é£Ÿå“è©³ç´°è³‡è¨Š"""
        detailed_foods = []
        
        try:
            logger.info(f"é–‹å§‹æŠ“å– {min(len(foods), max_details)} ç­†é£Ÿå“è©³ç´°è³‡è¨Š...")
            
            for i, food in enumerate(foods[:max_details]):
                if food.get('è©³ç´°é é¢URL'):
                    logger.info(f"æ­£åœ¨æŠ“å– {food['æ¨£å“åç¨±']} çš„è©³ç´°è³‡è¨Š...")
                    
                    detail = self.get_food_detail(food['è©³ç´°é é¢URL'])
                    if detail:
                        food['è©³ç´°è³‡è¨Š'] = detail
                        detailed_foods.append(food)
                    
                    # é¿å…è«‹æ±‚éæ–¼é »ç¹
                    time.sleep(2)
            
            logger.info(f"æˆåŠŸæŠ“å– {len(detailed_foods)} ç­†è©³ç´°è³‡è¨Š")
            return detailed_foods
            
        except Exception as e:
            logger.error(f"æŠ“å–é£Ÿå“è©³ç´°è³‡è¨Šå¤±æ•—: {e}")
            return detailed_foods
    
    def save_to_json(self, data: List[Dict], filename: str):
        """å„²å­˜è³‡æ–™ç‚º JSON æ ¼å¼"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"è³‡æ–™å·²å„²å­˜è‡³ {filename}")
            
        except Exception as e:
            logger.error(f"å„²å­˜ JSON æª”æ¡ˆå¤±æ•—: {e}")
    
    def save_to_csv(self, data: List[Dict], filename: str):
        """å„²å­˜è³‡æ–™ç‚º CSV æ ¼å¼"""
        try:
            if not data:
                return
            
            # å–å¾—æ‰€æœ‰æ¬„ä½
            fieldnames = set()
            for item in data:
                fieldnames.update(item.keys())
            
            fieldnames = sorted(list(fieldnames))
            
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(data)
            
            logger.info(f"è³‡æ–™å·²å„²å­˜è‡³ {filename}")
            
        except Exception as e:
            logger.error(f"å„²å­˜ CSV æª”æ¡ˆå¤±æ•—: {e}")
    
    def convert_to_nutrition_db(self, foods: List[Dict]) -> Dict:
        """è½‰æ›ç‚ºç‡Ÿé¤Šè³‡æ–™åº«æ ¼å¼"""
        nutrition_db = {}
        
        try:
            for food in foods:
                food_name = food.get('æ¨£å“åç¨±', '').lower()
                if not food_name:
                    continue
                
                # æå–ç‡Ÿé¤Šè³‡è¨Š
                nutrition = food.get('è©³ç´°è³‡è¨Š', {}).get('ç‡Ÿé¤Šæˆåˆ†', {})
                
                if nutrition:
                    # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
                    nutrition_db[food_name] = {
                        'calories': nutrition.get('ç†±é‡', 0),
                        'protein': nutrition.get('ç²—è›‹ç™½', 0),
                        'carbs': nutrition.get('ç¢³æ°´åŒ–åˆç‰©', 0),
                        'fat': nutrition.get('ç²—è„‚è‚ª', 0),
                        'fiber': nutrition.get('è†³é£Ÿçº–ç¶­', 0),
                        'vitamins': self.extract_vitamins(nutrition),
                        'minerals': self.extract_minerals(nutrition),
                        'source': 'FDA_TW',
                        'original_name': food.get('æ¨£å“åç¨±', ''),
                        'category': food.get('åˆ†é¡', '')
                    }
            
            logger.info(f"æˆåŠŸè½‰æ› {len(nutrition_db)} ç­†ç‡Ÿé¤Šè³‡æ–™")
            return nutrition_db
            
        except Exception as e:
            logger.error(f"è½‰æ›ç‡Ÿé¤Šè³‡æ–™åº«å¤±æ•—: {e}")
            return nutrition_db
    
    def extract_vitamins(self, nutrition: Dict) -> List[str]:
        """æå–ç¶­ç”Ÿç´ è³‡è¨Š"""
        vitamins = []
        vitamin_keywords = ['ç¶­ç”Ÿç´ A', 'ç¶­ç”Ÿç´ B', 'ç¶­ç”Ÿç´ C', 'ç¶­ç”Ÿç´ D', 'ç¶­ç”Ÿç´ E', 'ç¶­ç”Ÿç´ K']
        
        for key in nutrition.keys():
            for vitamin in vitamin_keywords:
                if vitamin in key:
                    vitamins.append(vitamin)
                    break
        
        return list(set(vitamins))
    
    def extract_minerals(self, nutrition: Dict) -> List[str]:
        """æå–ç¤¦ç‰©è³ªè³‡è¨Š"""
        minerals = []
        mineral_keywords = ['éˆ£', 'éµ', 'é‚', 'é‹…', 'é‰€', 'éˆ‰', 'ç£·']
        
        for key in nutrition.keys():
            for mineral in mineral_keywords:
                if mineral in key:
                    minerals.append(mineral)
                    break
        
        return list(set(minerals))

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ½ï¸ å°ç£FDAé£Ÿå“ç‡Ÿé¤Šæˆåˆ†è³‡æ–™åº«æŠ“å–å™¨")
    print("=" * 50)
    
    scraper = FDANutritionScraper()
    
    # æŠ“å–åŸºæœ¬è³‡æ–™
    print("ğŸ“‹ æŠ“å–é£Ÿå“åŸºæœ¬è³‡æ–™...")
    foods = scraper.scrape_all_foods(max_pages=10)  # é™åˆ¶é æ•¸é¿å…éåº¦è«‹æ±‚
    
    if foods:
        # å„²å­˜åŸºæœ¬è³‡æ–™
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        scraper.save_to_json(foods, f"fda_foods_basic_{timestamp}.json")
        scraper.save_to_csv(foods, f"fda_foods_basic_{timestamp}.csv")
        
        # æŠ“å–è©³ç´°è³‡è¨Š
        print("ğŸ” æŠ“å–é£Ÿå“è©³ç´°ç‡Ÿé¤Šè³‡è¨Š...")
        detailed_foods = scraper.scrape_food_details(foods, max_details=50)  # é™åˆ¶æ•¸é‡
        
        if detailed_foods:
            # å„²å­˜è©³ç´°è³‡æ–™
            scraper.save_to_json(detailed_foods, f"fda_foods_detailed_{timestamp}.json")
            scraper.save_to_csv(detailed_foods, f"fda_foods_detailed_{timestamp}.csv")
            
            # è½‰æ›ç‚ºç‡Ÿé¤Šè³‡æ–™åº«æ ¼å¼
            print("ğŸ”„ è½‰æ›ç‚ºç‡Ÿé¤Šè³‡æ–™åº«æ ¼å¼...")
            nutrition_db = scraper.convert_to_nutrition_db(detailed_foods)
            
            if nutrition_db:
                scraper.save_to_json(nutrition_db, f"fda_nutrition_db_{timestamp}.json")
                print(f"âœ… æˆåŠŸå»ºç«‹ç‡Ÿé¤Šè³‡æ–™åº«ï¼ŒåŒ…å« {len(nutrition_db)} ç¨®é£Ÿå“")
        
        print("ğŸ‰ è³‡æ–™æŠ“å–å®Œæˆï¼")
    else:
        print("âŒ æœªæŠ“å–åˆ°ä»»ä½•è³‡æ–™")

if __name__ == "__main__":
    main() 